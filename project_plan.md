# 基于 ASAP 的全身控制跨障任务方案

> 目标：在 ASAP（https://github.com/LeCAR-Lab/ASAP）的框架与流程基础上，训练一个全身控制模型，使机器人在地面存在障碍物/杂物时能够稳定跨越；演示开始时提供可行的脚落脚点、质心（CoM）与头部位置作为条件/引导。

## 1. 任务定义与假设

**任务描述**
- 场景：平地上分布若干可跨越的障碍物（盒子、木块、台阶、杂物等）。
- 行为：机器人跨过障碍物并保持稳定，不碰撞、不失稳。
- 先验信息：在演示/执行开始时提供“跨越所需的脚落脚点、质心、头部位置”。

**关键假设**
- 使用 ASAP 的训练流程（运动先验 + RL / 模仿 + 任务奖励）作为骨架。
- 机器人模型与仿真平台与 ASAP 保持一致（或可无缝替换）。

## 2. 系统设计概览

**核心模块**
1. **环境构建**：障碍物分布生成器、地形与可行步态区域。
2. **感知/状态编码**：障碍物几何 + 足端目标 + CoM/头部先验。
3. **控制策略**：全身控制策略网络（输出足端/关节动作或足端轨迹与身体姿态）。
4. **奖励设计**：稳定、跨越成功、轨迹跟踪、碰撞惩罚、能耗惩罚。
5. **课程学习**：从单个低障碍 → 多个障碍 → 随机障碍。

**输入/输出**
- **输入**：
  - 机器人本体状态（关节角、速度、足端位置、IMU）。
  - 障碍物局部几何/栅格（高度场或点云简化）。
  - 用户提供的落脚点、CoM、头部期望位置（序列或关键帧）。
- **输出**：
  - 关节目标（PD/低层控制器）。
  - 或足端/身体轨迹 + 低层 WBC。

## 3. ASAP 流程对齐

**ASAP 中关键思想**（建议对齐的地方）：
- 运动先验/参考轨迹 → 作为策略学习的“动作先验”。
- 通过任务奖励调整参考运动，实现目标导向的全身控制。

**具体对齐方案**
- 使用示范数据或规划结果生成“跨越动作片段”。
- 将“落脚点、CoM、头部位置”作为条件输入，类似于动作先验的引导信号。
- 训练中混合使用 imitation loss 与 task reward。

## 4. 数据与先验生成

**落脚点/CoM/头部轨迹来源**
1. **手工或规划器生成**：
   - 通过几何规则或简化规划器生成落脚点（起落步）。
   - CoM/头部轨迹平滑插值。
2. **示范数据**：
   - 通过既有动作片段（跨越动作）进行筛选与标注。
3. **混合策略**：
   - 先用规划器生成，再用 RL 微调。

## 5. 奖励函数建议

- **跨越成功奖励**：越过障碍物后稳定站立。
- **足端落脚点跟踪**：与目标落脚点距离惩罚。
- **CoM/头部跟踪**：与目标轨迹/关键帧误差惩罚。
- **碰撞惩罚**：足端、腿部、身体与障碍物碰撞惩罚。
- **稳定性**：倾倒、过大姿态角惩罚。
- **能耗**：关节力矩与速度惩罚。

## 6. 课程学习策略

1. **阶段 1**：低障碍 + 固定落脚点 → 训练基本跨越动作。
2. **阶段 2**：障碍随机位置/高度 + 落脚点扰动。
3. **阶段 3**：多障碍/复杂杂物 + 头部/CoM条件变化。

## 7. 评估指标

- 成功率（跨越无碰撞、无跌倒）。
- 轨迹跟踪误差（足端/CoM/头部）。
- 能耗、时间、最大姿态角。
- 泛化能力（新障碍布局）。

## 8. 实施步骤建议（最小可行版本）

1. **环境与障碍物生成**：实现随机障碍物、可视化。
2. **输入/输出接口**：将落脚点/CoM/头部作为条件输入。
3. **奖励函数**：添加轨迹跟踪与碰撞惩罚。
4. **训练与调参**：从简单障碍开始课程学习。
5. **评估与可视化**：录制跨越演示与成功率。

## 9. 已确认/需进一步确认的问题

- 机器人平台：后续可对齐 A1/Go1/ANYmal；当前以 **Isaac Gym → MuJoCo** 的 sim-to-sim 迁移为目标。
- 跨越动作示范或规划器：**当前没有现成示范/规划器**，需在 ASAP 流程中补充（如几何启发式脚步规划 + 轨迹插值）。
- 落脚点/CoM/头部位置：倾向使用**关键帧**（便于稀疏约束与规划），训练中可插值为连续轨迹。
- 实时性与部署：优先在仿真中完成训练与迁移，再考虑真实部署。

---

如果你愿意，我可以在此基础上进一步（已确认可继续推进）：
- 根据 ASAP 代码结构提供具体模块改动清单。
- 给出策略网络结构（条件编码、动作头）建议。
- 设计训练脚本与配置模板。
